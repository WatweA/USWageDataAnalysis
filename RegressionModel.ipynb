{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Regression Models to Predict Income\n",
    "## **A Notebook for Using Demographic Data to Predict Income/Wage**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data from pickles, and separate into the features/targets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(message):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S -\"), message)\n",
    "    \n",
    "def printnow():\n",
    "    print(datetime.now().strftime(\"Current time: %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "22:42:00 - Reading data\n22:42:16 - Done\n\n"
    }
   ],
   "source": [
    "log('Reading data')\n",
    "us_personal = pd.read_pickle('preprocessed_data/onehot_data.zip')\n",
    "log('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler # a scaler to overcome outliers from the data\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor # baseline regression models\n",
    "from keras import Sequential # all relevant imports for Keras (TensorFlow based neural net)\n",
    "from keras.layers import Dense, LeakyReLU, ELU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# metrics to exaluate the performance of our regressors\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "22:42:19 - Splitting Train/Test Data\n22:42:40 - Scaling Train/Test Features\n22:43:06 - Done\n\n"
    }
   ],
   "source": [
    "# Separate the data into three groups: incomes or wages (targets), and features (all other columns)\n",
    "# and return a corresponding dictionary with the three sets\n",
    "def features_targets(df):\n",
    "    features = df.drop(columns=['PINCP', 'WAGP'])\n",
    "    incomes = df['PINCP']\n",
    "    wages = df['WAGP']\n",
    "    return {'features':features, 'incomes':incomes, 'wages':wages}\n",
    "\n",
    "# create a dictionary representing the three sets and their features/targets\n",
    "log('Splitting Train/Test Data')\n",
    "train, test = train_test_split(us_personal, test_size=0.2)\n",
    "dataset = {\n",
    "    'train':features_targets(train),\n",
    "    'test':features_targets(test)}\n",
    "\n",
    "\n",
    "# Initialize the RobustScaler on the training data (fit before training each regression model)\n",
    "log('Scaling Train/Test Features')\n",
    "robust_scaler = RobustScaler().fit(dataset['train']['features'])\n",
    "dataset['train']['features'] = robust_scaler.transform(dataset['train']['features'])\n",
    "dataset['test']['features'] = robust_scaler.transform(dataset['test']['features'])\n",
    "log('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to fit, test, and print results of a given estimator for a given target column\n",
    "def fit_test_print(estimator, estimator_name, target, target_name, grid=1):\n",
    "    log(f'Training {estimator_name}')\n",
    "    estimator.fit(dataset['train']['features'], dataset['train'][target].array)\n",
    "    y_pred_train = estimator.predict(dataset['train']['features'])\n",
    "    y_pred_test = estimator.predict(dataset['test']['features'])\n",
    "    log('Done\\n')\n",
    "    \n",
    "    print(f\"Results for {target_name} with {estimator_name}\")\n",
    "    if grid == 1: print(f\"- Best parameters: {estimator.best_params_}\")\n",
    "\n",
    "    print(f\"- Training Set\")\n",
    "    print(f\"\\tMean Squared Error: {mean_squared_error(dataset['train'][target], y_pred_train)}\")\n",
    "    print(f\"\\tMedian Absolute Error: {median_absolute_error(dataset['train'][target], y_pred_train)}\")\n",
    "    print(f\"\\tr-Squared: {r2_score(dataset['train'][target], y_pred_train)}\")\n",
    "\n",
    "    print(f\"- Test Set\")\n",
    "    print(f\"\\tMean Squared Error: {mean_squared_error(dataset['test'][target], y_pred_test)}\")\n",
    "    print(f\"\\tMedian Absolute Error: {median_absolute_error(dataset['test'][target], y_pred_test)}\")\n",
    "    print(f\"\\tr-Squared: {r2_score(dataset['test'][target], y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Implemented with ElasticNet, which combines L1 and L2 regularization"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_params = {\n",
    "    'alpha':[10, 100, 1000, 10000],\n",
    "    'l1_ratio':[0.40, 0.60]\n",
    "}\n",
    "\n",
    "linear_model = GridSearchCV(estimator=ElasticNet(), param_grid=linear_model_params, \n",
    "                            scoring=['neg_mean_absolute_error', 'r2'], \n",
    "                            refit='r2', cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16:14:08 - Training Linear Model L1/L2 Regularized\n16:26:53 - Done\n\nResults for Income with Linear Model L1/L2 Regularized\n- Best parameters: {'alpha': 10, 'l1_ratio': 0.6}\n- Training Set\n\tMean Squared Error: 3274596194.857993\n\tMedian Absolute Error: 22929.83070571476\n\tr-Squared: 0.11891055597779776\n- Test Set\n\tMean Squared Error: 3281850782.70632\n\tMedian Absolute Error: 22924.74442348032\n\tr-Squared: 0.11853319389596961\n"
    }
   ],
   "source": [
    "fit_test_print(linear_model, 'Linear Model L1/L2 Regularized', 'incomes', 'Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16:29:04 - Training Linear Model L1/L2 Regularized\n16:37:42 - Done\n\nResults for Wage with Linear Model L1/L2 Regularized\n- Best parameters: {'alpha': 10, 'l1_ratio': 0.6}\n- Training Set\n\tMean Squared Error: 2351956604.698796\n\tMedian Absolute Error: 17558.869250567495\n\tr-Squared: 0.15201561556297183\n- Test Set\n\tMean Squared Error: 2333282440.602135\n\tMedian Absolute Error: 17562.433428652887\n\tr-Squared: 0.15276740461628124\n"
    }
   ],
   "source": [
    "fit_test_print(linear_model, 'Linear Model L1/L2 Regularized', 'wages', 'Wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Regression\n",
    "Loss set to Huber to be more robust to outliers in income and wage"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_params = {\n",
    "    'alpha':[0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "sgd_model = GridSearchCV(estimator=SGDRegressor(penalty='elasticnet', learning_rate='adaptive'), param_grid=sgd_model_params, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "22:43:06 - Training SGD Model L1/L2 Regularized\n23:35:27 - Done\n\nResults for Income with SGD Model L1/L2 Regularized\n- Best parameters: {'alpha': 0.0001}\n- Training Set\n\tMean Squared Error: 2313730116.8329206\n\tMedian Absolute Error: 12513.364387014095\n\tr-Squared: 0.3787176688793734\n- Test Set\n\tMean Squared Error: 2288257245.7425137\n\tMedian Absolute Error: 12522.970271169928\n\tr-Squared: 0.380348437367074\n"
    }
   ],
   "source": [
    "fit_test_print(sgd_model, 'SGD Model L1/L2 Regularized', 'incomes', 'Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PINCP             0\n0       12000.0  26969.389940\n1           0.0  -4974.186043\n2           0.0  -9717.373729\n3       40000.0  77796.252504\n4       13000.0  57601.649867\n...         ...           ...\n642903      0.0  12223.399332\n642904      0.0   1882.580636\n642905  44000.0  65213.996765\n642906      0.0  -1778.753408\n642907      0.0 -18568.921707\n\n[642908 rows x 2 columns]\n          PINCP         0\nPINCP  1.000000  0.616728\n0      0.616728  1.000000\n"
    }
   ],
   "source": [
    "income_pred = pd.Series(sgd_model.predict(dataset['test']['features']))\n",
    "df = pd.concat([dataset['test']['incomes'].reset_index(drop=True), income_pred], axis=1)\n",
    "print(df)\n",
    "print(df.corr())\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23:35:28 - Training Logistic Model L1/L2 Regularized\n00:21:39 - Done\n\nResults for Wage with Logistic Model L1/L2 Regularized\n- Best parameters: {'alpha': 0.0001}\n- Training Set\n\tMean Squared Error: 1565194628.2093012\n\tMedian Absolute Error: 9285.146097658673\n\tr-Squared: 0.4363179455367291\n- Test Set\n\tMean Squared Error: 1536207636.0796754\n\tMedian Absolute Error: 9283.257766323872\n\tr-Squared: 0.4396291032552606\n"
    }
   ],
   "source": [
    "fit_test_print(sgd_model, 'Logistic Model L1/L2 Regularized', 'wages', 'Wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Net Regression\n",
    "Implemented with feed-forward dense layers "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "list"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "type(list(dataset['train']['incomes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to initialize the model\n",
    "def init_keras():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=151))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(200))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(50))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(20))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='huber_loss', optimizer='adam', metrics=['mean_absolute_error', 'cosine_proximity'])\n",
    "    return model\n",
    "\n",
    "neural_net_model = KerasRegressor(build_fn=init_keras, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "00:28:59 - Training Keras Neural Net\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Please provide as model targets either a single array or a list of arrays. You passed: y=<PandasArray>\n[21300.0,     0.0, 67600.0,     0.0,     0.0, 25000.0, 82000.0, 17000.0,\n 30000.0,     0.0,\n ...\n     0.0, 53000.0,     0.0,     0.0,     0.0,  1300.0,  9800.0, 55000.0,\n     0.0,     0.0]\nLength: 2571631, dtype: float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9cc313181b30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit_test_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Keras Neural Net'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'incomes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Income'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4c1e0ff5d9aa>\u001b[0m in \u001b[0;36mfit_test_print\u001b[1;34m(estimator, estimator_name, target, target_name, grid)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfit_test_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training {estimator_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    527\u001b[0m                                          \u001b[1;34m'either a single '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m                                          \u001b[1;34m'array or a list of arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m                                          'You passed: y=' + str(y))\n\u001b[0m\u001b[0;32m    530\u001b[0m                 \u001b[1;31m# Typecheck that all inputs are *either* value *or* symbolic.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide as model targets either a single array or a list of arrays. You passed: y=<PandasArray>\n[21300.0,     0.0, 67600.0,     0.0,     0.0, 25000.0, 82000.0, 17000.0,\n 30000.0,     0.0,\n ...\n     0.0, 53000.0,     0.0,     0.0,     0.0,  1300.0,  9800.0, 55000.0,\n     0.0,     0.0]\nLength: 2571631, dtype: float64"
     ]
    }
   ],
   "source": [
    "fit_test_print(neural_net_model, 'Keras Neural Net', 'incomes', 'Income', grid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_test_print(neural_net_model, 'Keras Neural Net', 'wages', 'Wage', grid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}